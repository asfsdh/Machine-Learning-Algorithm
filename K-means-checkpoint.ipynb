{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlC: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@8007be6\n",
       "import sqlC.implicits._\n",
       "data: org.apache.spark.sql.DataFrame = [duration: int, protocol_type: string ... 40 more fields]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlC.implicits._\n",
    "\n",
    "val data = spark.read.\n",
    "option( \"inferSchema\" , true) .\n",
    "option( \"header\" , false).\n",
    "csv( \"Data/kdd/kddcup.data_10_percent_corrected\" ).\n",
    "toDF(\n",
    "\"duration\", \"protocol_type\" , \"service\", \"flag\" ,\n",
    "\"src_bytes\" , \"dst_bytes\" , \"land\" , \"wrong_fragment\" , \"urgent\" ,\n",
    "\"hot\" , \"num_failed_logins\" , \"logged_in\", \"num_compromised\" ,\n",
    "\"root_shell\", \"su_attempted\" , \"num_root\" , \"num_file_creations\" ,\n",
    "\"num_shells\",\"num_access_files\" , \"num_out bound_cmds\" ,\n",
    "\"is_host_login\", \"is_guest_login\" , \"count\" , \"srv_count\",\n",
    "\"serror_rate\" , \"srv_serror_rate\" , \"rerror_rate\" , \"srv_rerror_rate\",\n",
    "\"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\" ,\n",
    "\" dst_host_count\", \"dst_host_srv_count \" ,\n",
    "\"dst_host_same_srv_rate\" , \"dst_host_di ff_srv_rate\" ,\n",
    "\"dst_host_same_src_port_rate\" , \"dst_host_srv_diff_host_rate\" ,\n",
    "\"dst_host_serror_rate\" , \"dst_host_srv_serror_rate\" ,\n",
    "\"dst_host_rerror_rate\" , \"dst_host_srv_rerror_rate\",\n",
    "\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: data.type = [duration: int, protocol_type: string ... 40 more fields]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|280790|\n",
      "|        neptune.|107201|\n",
      "|         normal.| 97278|\n",
      "|           back.|  2203|\n",
      "|          satan.|  1589|\n",
      "|        ipsweep.|  1247|\n",
      "|      portsweep.|  1040|\n",
      "|    warezclient.|  1020|\n",
      "|       teardrop.|   979|\n",
      "|            pod.|   264|\n",
      "|           nmap.|   231|\n",
      "|   guess_passwd.|    53|\n",
      "|buffer_overflow.|    30|\n",
      "|           land.|    21|\n",
      "|    warezmaster.|    20|\n",
      "|           imap.|    12|\n",
      "|        rootkit.|    10|\n",
      "|     loadmodule.|     9|\n",
      "|      ftp_write.|     8|\n",
      "|       multihop.|     7|\n",
      "|            phf.|     4|\n",
      "|           perl.|     3|\n",
      "|            spy.|     2|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"label\").groupBy(\"label\").count().orderBy($\"count\".desc).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numericOnly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [duration: int, src_bytes: int ... 37 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numericOnly = data.drop(\"protocol_type\" , \"service\", \"flag\" ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
       "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
       "import org.apache.spark.ml.feature.{OneHotEncoder, VectorAssembler, StringIndexer, StandardScaler}\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import scala.util.Random\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{PipelineModel,Pipeline}\n",
    "import org.apache.spark.ml.clustering.{KMeans,KMeansModel}\n",
    "import org.apache.spark.ml.feature.{OneHotEncoder,VectorAssembler,StringIndexer,StandardScaler}\n",
    "import org.apache.spark.ml.linalg.{Vector,Vectors}\n",
    "import org.apache.spark.sql .{DataFrame,SparkSession}\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_7697d6ef047e, handleInvalid=error, numInputCols=38\n",
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_6dba055f4a56\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_252f1faef639\n",
       "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_252f1faef639\n",
       "kmeansModel: org.apache.spark.ml.clustering.KMeansModel = KMeansModel: uid=kmeans_6dba055f4a56, k=2, distanceMeasure=euclidean, numFeatures=38\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "setInputCols(numericOnly.columns.filter(_!=\"label\")).\n",
    "setOutputCol( \"featureVector\" )\n",
    "\n",
    "val kmeans = new KMeans().\n",
    "setSeed(Random.nextLong( )).\n",
    "setPredictionCol( \"cluster\" ).\n",
    "setFeaturesCol( \"featureVector\" )\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler,kmeans))\n",
    "val pipelineModel = pipeline.fit(numericOnly)\n",
    "val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.979395571029514,1622.078830816566,868.5341828266062,4.453261001578883E-5,0.006432937937735314,1.4169466823205539E-5,0.03451682118132869,1.5181571596291647E-4,0.14824703453301485,0.01021213716043885,1.1133152503947209E-4,3.6435771831099954E-5,0.011351767134933808,0.0010829521072021374,1.0930731549329986E-4,0.0010080563539937655,0.0,0.0,0.0013865835391279706,332.2862475203433,292.9071434354884,0.17668541759442943,0.17660780940042914,0.05743309987449898,0.05771839196793656,0.7915488441762945,0.020981640419421355,0.028996862475203923,232.4707319541719,188.6660459090725,0.7537812031901686,0.030905611108870867,0.6019355289259973,0.006683514837454898,0.17675395732966057,0.1764416217966883,0.05811762681672766,0.057411116958826745]\n",
      "[2.0,6.9337564E8,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,57.0,3.0,0.79,0.67,0.21,0.33,0.05,0.39,0.0,255.0,3.0,0.01,0.09,0.22,0.0,0.18,0.67,0.05,0.33]\n"
     ]
    }
   ],
   "source": [
    "kmeansModel.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+\n",
      "|cluster|           label| count|\n",
      "+-------+----------------+------+\n",
      "|      0|          smurf.|280790|\n",
      "|      0|        neptune.|107201|\n",
      "|      0|         normal.| 97278|\n",
      "|      0|           back.|  2203|\n",
      "|      0|          satan.|  1589|\n",
      "|      0|        ipsweep.|  1247|\n",
      "|      0|      portsweep.|  1039|\n",
      "|      0|    warezclient.|  1020|\n",
      "|      0|       teardrop.|   979|\n",
      "|      0|            pod.|   264|\n",
      "|      0|           nmap.|   231|\n",
      "|      0|   guess_passwd.|    53|\n",
      "|      0|buffer_overflow.|    30|\n",
      "|      0|           land.|    21|\n",
      "|      0|    warezmaster.|    20|\n",
      "|      0|           imap.|    12|\n",
      "|      0|        rootkit.|    10|\n",
      "|      0|     loadmodule.|     9|\n",
      "|      0|      ftp_write.|     8|\n",
      "|      0|       multihop.|     7|\n",
      "|      0|            phf.|     4|\n",
      "|      0|           perl.|     3|\n",
      "|      0|            spy.|     2|\n",
      "|      1|      portsweep.|     1|\n",
      "+-------+----------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withCluster: org.apache.spark.sql.DataFrame = [duration: int, src_bytes: int ... 39 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withCluster = pipelineModel.transform(numericOnly)\n",
    "withCluster.select( \"cluster\", \"label\" ).\n",
    "groupBy( \"cluster\", \"label\" ).count().\n",
    "orderBy($\"cluster\", $\"count\".desc).\n",
    "show(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "81: error: value computeCost is not a member of org.apache.spark.ml.clustering.KMeansModel",
     "output_type": "error",
     "traceback": [
      "<console>:81: error: value computeCost is not a member of org.apache.spark.ml.clustering.KMeansModel",
      "       kmeansModel.computeCost(assembler.transform(data))/data.count()",
      "                   ^",
      ""
     ]
    }
   ],
   "source": [
    "def clusteringScore0(data:DataFrame,k:Int):Double =\n",
    "{\n",
    "    \n",
    "val assembler = new VectorAssembler().\n",
    "setInputCols(data.columns.filter(_!= \"label\" )).\n",
    "setOutputCol( \"featureVector\")\n",
    "    \n",
    "val kmeans = new KMeans().\n",
    "setSeed( Random.nextLong()).\n",
    "setK(k).\n",
    "setPredictionCol( \"cluster\" ).\n",
    "setFeaturesCol( \"featureVector\" )\n",
    "    \n",
    "val pipeline = new Pipeline().setStages(Array(assembler,kmeans))\n",
    "    \n",
    "val kmeansModel = pipeline.fit(data).stages.last.asInstanceOf[KMeansModel]\n",
    "    \n",
    "kmeansModel.computeCost(assembler.transform(data))/data.count()\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
