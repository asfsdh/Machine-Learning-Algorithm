{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1601079793472)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
       "import org.apache.spark.ml.classification.{DecisionTreeClassifier, RandomForestClassifier, RandomForestClassificationModel}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "import scala.util.Random\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{PipelineModel,Pipeline}\n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier,RandomForestClassifier,RandomForestClassificationModel}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,VectorIndexer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder,TrainValidationSplit}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.sql.{DataFrame,SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataWithoutHeader = spark.read.option(\"inferSchema\", true).option(\"header\",false).csv(\"Data/cov/covtype.data\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.Row = [2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataWithoutHeader.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: integer (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: integer (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: integer (nullable = true)\n",
      " |-- _c34: integer (nullable = true)\n",
      " |-- _c35: integer (nullable = true)\n",
      " |-- _c36: integer (nullable = true)\n",
      " |-- _c37: integer (nullable = true)\n",
      " |-- _c38: integer (nullable = true)\n",
      " |-- _c39: integer (nullable = true)\n",
      " |-- _c40: integer (nullable = true)\n",
      " |-- _c41: integer (nullable = true)\n",
      " |-- _c42: integer (nullable = true)\n",
      " |-- _c43: integer (nullable = true)\n",
      " |-- _c44: integer (nullable = true)\n",
      " |-- _c45: integer (nullable = true)\n",
      " |-- _c46: integer (nullable = true)\n",
      " |-- _c47: integer (nullable = true)\n",
      " |-- _c48: integer (nullable = true)\n",
      " |-- _c49: integer (nullable = true)\n",
      " |-- _c50: integer (nullable = true)\n",
      " |-- _c51: integer (nullable = true)\n",
      " |-- _c52: integer (nullable = true)\n",
      " |-- _c53: integer (nullable = true)\n",
      " |-- _c54: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataWithoutHeader.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colNames: Seq[String] = List(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Yertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, HilIshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soil_..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNames =Seq(\n",
    "\"Elevation\", \"Aspect\", \"Slope\" ,\n",
    "\"Horizontal_Distance_To_Hydrology\",\"Yertical_Distance_To_Hydrology\" ,\n",
    "\"Horizontal_Distance_To_Roadways\",\n",
    "\"HilIshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\n",
    "\"Horizontal_Distance_To_Fire_Points\")++\n",
    "((0 until 4 ).map(i => s\"Wilderness_Area_$i\"))++\n",
    "((0 until 40).map(i => s\"Soil_Type_$i\"))++Seq(\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data =dataWithoutHeader.toDF(colNames:_* ).withColumn(\"Cover_Type\",$\"Cover_Type\".cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Yertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- HilIshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_0: integer (nullable = true)\n",
      " |-- Wilderness_Area_1: integer (nullable = true)\n",
      " |-- Wilderness_Area_2: integer (nullable = true)\n",
      " |-- Wilderness_Area_3: integer (nullable = true)\n",
      " |-- Soil_Type_0: integer (nullable = true)\n",
      " |-- Soil_Type_1: integer (nullable = true)\n",
      " |-- Soil_Type_2: integer (nullable = true)\n",
      " |-- Soil_Type_3: integer (nullable = true)\n",
      " |-- Soil_Type_4: integer (nullable = true)\n",
      " |-- Soil_Type_5: integer (nullable = true)\n",
      " |-- Soil_Type_6: integer (nullable = true)\n",
      " |-- Soil_Type_7: integer (nullable = true)\n",
      " |-- Soil_Type_8: integer (nullable = true)\n",
      " |-- Soil_Type_9: integer (nullable = true)\n",
      " |-- Soil_Type_10: integer (nullable = true)\n",
      " |-- Soil_Type_11: integer (nullable = true)\n",
      " |-- Soil_Type_12: integer (nullable = true)\n",
      " |-- Soil_Type_13: integer (nullable = true)\n",
      " |-- Soil_Type_14: integer (nullable = true)\n",
      " |-- Soil_Type_15: integer (nullable = true)\n",
      " |-- Soil_Type_16: integer (nullable = true)\n",
      " |-- Soil_Type_17: integer (nullable = true)\n",
      " |-- Soil_Type_18: integer (nullable = true)\n",
      " |-- Soil_Type_19: integer (nullable = true)\n",
      " |-- Soil_Type_20: integer (nullable = true)\n",
      " |-- Soil_Type_21: integer (nullable = true)\n",
      " |-- Soil_Type_22: integer (nullable = true)\n",
      " |-- Soil_Type_23: integer (nullable = true)\n",
      " |-- Soil_Type_24: integer (nullable = true)\n",
      " |-- Soil_Type_25: integer (nullable = true)\n",
      " |-- Soil_Type_26: integer (nullable = true)\n",
      " |-- Soil_Type_27: integer (nullable = true)\n",
      " |-- Soil_Type_28: integer (nullable = true)\n",
      " |-- Soil_Type_29: integer (nullable = true)\n",
      " |-- Soil_Type_30: integer (nullable = true)\n",
      " |-- Soil_Type_31: integer (nullable = true)\n",
      " |-- Soil_Type_32: integer (nullable = true)\n",
      " |-- Soil_Type_33: integer (nullable = true)\n",
      " |-- Soil_Type_34: integer (nullable = true)\n",
      " |-- Soil_Type_35: integer (nullable = true)\n",
      " |-- Soil_Type_36: integer (nullable = true)\n",
      " |-- Soil_Type_37: integer (nullable = true)\n",
      " |-- Soil_Type_38: integer (nullable = true)\n",
      " |-- Soil_Type_39: integer (nullable = true)\n",
      " |-- Cover_Type: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainData,testData) = data.randomSplit(Array(0.9,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: testData.type = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.cache( )\n",
    "testData.cache( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Yertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, HilIshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, S..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputCols = trainData.columns. filter(_!=\"Cover_Type\")\n",
    "val assembler = new VectorAssembler().setInputCols(inputCols).setOutputCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1859.0,18.0,12.0,67.0,11.0,90.0,211.0,215.0,139.0,792.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1860.0,18.0,13.0,95.0,15.0,90.0,210.0,213.0,138.0,780.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1861.0,35.0,14.0,60.0,11.0,85.0,218.0,209.0,124.0,832.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1866.0,23.0,14.0,85.0,16.0,108.0,212.0,210.0,133.0,819.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1867.0,20.0,15.0,108.0,19.0,120.0,208.0,206.0,132.0,808.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1868.0,27.0,16.0,67.0,17.0,95.0,212.0,204.0,125.0,859.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1871.0,22.0,22.0,60.0,12.0,85.0,200.0,187.0,115.0,792.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1871.0,37.0,19.0,120.0,29.0,90.0,216.0,195.0,107.0,759.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1872.0,12.0,27.0,85.0,25.0,60.0,182.0,174.0,118.0,577.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1872.0,27.0,16.0,95.0,22.0,124.0,212.0,205.0,126.0,847.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1872.0,27.0,21.0,108.0,30.0,67.0,206.0,190.0,112.0,713.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1872.0,35.0,21.0,120.0,18.0,85.0,213.0,189.0,104.0,797.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1873.0,30.0,19.0,67.0,21.0,85.0,211.0,195.0,114.0,899.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1876.0,25.0,17.0,124.0,26.0,150.0,209.0,200.0,123.0,836.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1876.0,29.0,19.0,124.0,34.0,90.0,210.0,195.0,115.0,750.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1877.0,27.0,24.0,90.0,18.0,95.0,201.0,179.0,104.0,780.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1877.0,28.0,22.0,127.0,35.0,85.0,205.0,185.0,107.0,706.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1879.0,18.0,14.0,120.0,208.0,210.0,137.0,767.0,1.0,1.0])               |\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assembledTrainData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 54 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembledTrainData = assembler.transform(trainData)\n",
    "assembledTrainData.select(\"featureVector\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_2ef5601cc8d0\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifier = new DecisionTreeClassifier().\n",
    "setSeed(Random.nextLong( )).\n",
    "setLabelCol(\"Cover_Type\").\n",
    "setFeaturesCol(\"featureVector\").\n",
    "setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_2ef5601cc8d0) of depth 5 with 47 nodes\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model=classifier.fit(assembledTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=dtc_2ef5601cc8d0) of depth 5 with 47 nodes\n",
      "  If (feature 0 <= 3035.5)\n",
      "   If (feature 0 <= 2559.5)\n",
      "    If (feature 10 <= 0.5)\n",
      "     If (feature 0 <= 2440.5)\n",
      "      If (feature 3 <= 15.0)\n",
      "       Predict: 4.0\n",
      "      Else (feature 3 > 15.0)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2440.5)\n",
      "      Predict: 3.0\n",
      "    Else (feature 10 > 0.5)\n",
      "     If (feature 9 <= 5308.0)\n",
      "      Predict: 2.0\n",
      "     Else (feature 9 > 5308.0)\n",
      "      If (feature 5 <= 559.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 5 > 559.0)\n",
      "       Predict: 5.0\n",
      "   Else (feature 0 > 2559.5)\n",
      "    If (feature 15 <= 0.5)\n",
      "     If (feature 0 <= 2920.5)\n",
      "      If (feature 17 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 17 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2920.5)\n",
      "      Predict: 2.0\n",
      "    Else (feature 15 > 0.5)\n",
      "     If (feature 9 <= 1425.5)\n",
      "      If (feature 7 <= 218.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 7 > 218.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 9 > 1425.5)\n",
      "      If (feature 5 <= 2270.5)\n",
      "       Predict: 3.0\n",
      "      Else (feature 5 > 2270.5)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3035.5)\n",
      "   If (feature 0 <= 3304.5)\n",
      "    If (feature 7 <= 238.5)\n",
      "     If (feature 0 <= 3101.5)\n",
      "      If (feature 3 <= 191.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 191.0)\n",
      "       Predict: 2.0\n",
      "     Else (feature 0 > 3101.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 7 > 238.5)\n",
      "     If (feature 3 <= 316.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 316.0)\n",
      "      If (feature 0 <= 3201.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3201.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3304.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      If (feature 0 <= 3356.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 0 > 3356.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 1007.0)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 1007.0)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8307218141535029,Elevation)\n",
      "(0.031185642096493604,Wilderness_Area_0)\n",
      "(0.028559904177147016,Horizontal_Distance_To_Hydrology)\n",
      "(0.027727865227010973,Hillshade_Noon)\n",
      "(0.024663266650117532,Soil_Type_1)\n",
      "(0.02232787165927525,Soil_Type_3)\n",
      "(0.017293618203041206,Soil_Type_31)\n",
      "(0.01161827830496091,Wilderness_Area_2)\n",
      "(0.0037159618108047383,Horizontal_Distance_To_Roadways)\n",
      "(0.0021857777176459655,Horizontal_Distance_To_Fire_Points)\n",
      "(0.0,Yertical_Distance_To_Hydrology)\n",
      "(0.0,Wilderness_Area_3)\n",
      "(0.0,Wilderness_Area_1)\n",
      "(0.0,Soil_Type_9)\n",
      "(0.0,Soil_Type_8)\n",
      "(0.0,Soil_Type_7)\n",
      "(0.0,Soil_Type_6)\n",
      "(0.0,Soil_Type_5)\n",
      "(0.0,Soil_Type_4)\n",
      "(0.0,Soil_Type_39)\n",
      "(0.0,Soil_Type_38)\n",
      "(0.0,Soil_Type_37)\n",
      "(0.0,Soil_Type_36)\n",
      "(0.0,Soil_Type_35)\n",
      "(0.0,Soil_Type_34)\n",
      "(0.0,Soil_Type_33)\n",
      "(0.0,Soil_Type_32)\n",
      "(0.0,Soil_Type_30)\n",
      "(0.0,Soil_Type_29)\n",
      "(0.0,Soil_Type_28)\n",
      "(0.0,Soil_Type_27)\n",
      "(0.0,Soil_Type_26)\n",
      "(0.0,Soil_Type_25)\n",
      "(0.0,Soil_Type_24)\n",
      "(0.0,Soil_Type_23)\n",
      "(0.0,Soil_Type_22)\n",
      "(0.0,Soil_Type_21)\n",
      "(0.0,Soil_Type_20)\n",
      "(0.0,Soil_Type_2)\n",
      "(0.0,Soil_Type_19)\n",
      "(0.0,Soil_Type_18)\n",
      "(0.0,Soil_Type_17)\n",
      "(0.0,Soil_Type_16)\n",
      "(0.0,Soil_Type_15)\n",
      "(0.0,Soil_Type_14)\n",
      "(0.0,Soil_Type_13)\n",
      "(0.0,Soil_Type_12)\n",
      "(0.0,Soil_Type_11)\n",
      "(0.0,Soil_Type_10)\n",
      "(0.0,Soil_Type_0)\n",
      "(0.0,Slope)\n",
      "(0.0,Hillshade_3pm)\n",
      "(0.0,HilIshade_9am)\n",
      "(0.0,Aspect)\n"
     ]
    }
   ],
   "source": [
    "model.featureImportances.toArray.zip(inputCols).sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                      |\n",
      "+----------+----------+-------------------------------------------------------------------------------------------------+\n",
      "|3.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.04095409540954095,0.28622862286228623,0.44554455445544555,0.0,0.22727272727272727,0.0]|\n",
      "|3.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.02991944764096663,0.6351769275028769,0.052035385500575376,0.0,0.28286823935558114,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.04095409540954095,0.28622862286228623,0.44554455445544555,0.0,0.22727272727272727,0.0]|\n",
      "+----------+----------+-------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 57 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(assembledTrainData)\n",
    "predictions.select( \"Cover_Type\" , \"prediction\" , \"probability\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_ad06475e03bb\n",
       "accuracy: Double = 0.6962887768620636\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "setLabelCol(\"Cover_Type\").\n",
    "setPredictionCol(\"prediction\")\n",
    "\n",
    "val accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126580.0  62239.0   246.0    0.0    0.0    0.0  1654.0  \n",
      "46159.0   200770.0  7745.0   91.0   68.0   0.0  211.0   \n",
      "0.0       2348.0    29173.0  636.0  0.0    0.0  0.0     \n",
      "0.0       0.0       1473.0   990.0  0.0    0.0  0.0     \n",
      "0.0       7697.0    741.0    0.0    101.0  0.0  0.0     \n",
      "0.0       3108.0    12107.0  505.0  0.0    0.0  0.0     \n",
      "11764.0   40.0      36.0     0.0    0.0    0.0  6607.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictionRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[89] at rdd at <console>:40\n",
       "multiclassMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@4defa8eb\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionRDD =predictions.\n",
    "select( \"prediction\", \"Cover_Type\").\n",
    "as[(Double,Double)].rdd\n",
    "\n",
    "val multiclassMetrics = new MulticlassMetrics(predictionRDD)\n",
    "\n",
    "println(multiclassMetrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifierHyper: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_9b09ca612a3f\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_c704080c98f2\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifierHyper = new DecisionTreeClassifier().\n",
    "setSeed(Random.nextLong()).\n",
    "setLabelCol( \"Cover_Type\" ).\n",
    "setFeaturesCol( \"featureVector\" ).\n",
    "setPredictionCol ( \"predict ion\" )\n",
    "val pipeline = new Pipeline().setStages(Array(assembler,classifierHyper))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tdtc_2ef5601cc8d0-impurity: gini,\n",
       "\tdtc_2ef5601cc8d0-maxBins: 40,\n",
       "\tdtc_2ef5601cc8d0-maxDepth: 1,\n",
       "\tdtc_2ef5601cc8d0-minInfoGain: 0.0\n",
       "}, {\n",
       "\tdtc_2ef5601cc8d0-impurity: entropy,\n",
       "\tdtc_2ef5601cc8d0-maxBins: 40,\n",
       "\tdtc_2ef5601cc8d0-maxDepth: 1,\n",
       "\tdtc_2ef5601cc8d0-minInfoGain: 0.0\n",
       "}, {\n",
       "\tdtc_2ef5601cc8d0-impurity: gini,\n",
       "\tdtc_2ef5601cc8d0-maxBins: 40,\n",
       "\tdtc_2ef5601cc8d0-maxDepth: 1,\n",
       "\tdtc_2ef5601cc8d0-minInfoGain: 0.05\n",
       "}, {\n",
       "\tdtc_2ef5601cc8d0-impurity: entropy,\n",
       "\tdtc_2ef5601cc8d0-maxBins: 40,\n",
       "\tdtc_2ef5601cc8d0-maxDepth: 1,\n",
       "\tdtc_2ef5601cc8d0-minInfoGain: 0.05\n",
       "}, {\n",
       "\tdtc_2ef5601cc8d0-impurity: gini,\n",
       "\tdtc_2ef5601cc8d0-maxBins: 40,\n",
       "\tdtc_2ef5601cc8d0-maxDepth: 3,\n",
       "\tdtc_2ef5601cc8d0-minInfoGain: 0.0\n",
       "}, {\n",
       "\tdtc_2ef5601cc8d0-impurity: entropy,\n",
       "\tdtc_2ef5..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    ".addGrid(classifier.impurity, Seq(\"gini\", \"entropy\"))\n",
    ".addGrid(classifier.maxDepth, Seq(1, 3))\n",
    ".addGrid(classifier.maxBins, Seq(40, 55))\n",
    ".addGrid(classifier.minInfoGain, Seq(0.0, 0.05))\n",
    ".build()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiclassEval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_28c019c7ad94\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "setLabelCol(\"Cover_Type\").\n",
    "setPredictionCol(\"prediction\").\n",
    "setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validator: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_38e636b9226e\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val validator = new TrainValidationSplit().\n",
    "setSeed(Random.nextLong()).\n",
    "setEstimator(pipeline).\n",
    "setEvaluator(multiclassEval).\n",
    "setEstimatorParamMaps(paramGrid).\n",
    "setTrainRatio(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Exception thrown in awaitResult:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Exception thrown in awaitResult:",
      "  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1$$anonfun$5.apply(TrainValidationSplit.scala:164)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1$$anonfun$5.apply(TrainValidationSplit.scala:164)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1.apply(TrainValidationSplit.scala:164)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1.apply(TrainValidationSplit.scala:121)",
      "  at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)",
      "  at scala.util.Try$.apply(Try.scala:192)",
      "  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)",
      "  ... 38 elided",
      "Caused by: java.lang.IllegalArgumentException: Field \"prediction\" does not exist.",
      "Available fields: Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Yertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, HilIshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soil_Type_35, Soil_Type_36, Soil_Type_37, Soil_Type_38, Soil_Type_39, Cover_Type, featureVector, rawPrediction, probability, predict ion",
      "  at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)",
      "  at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)",
      "  at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)",
      "  at scala.collection.AbstractMap.getOrElse(Map.scala:59)",
      "  at org.apache.spark.sql.types.StructType.apply(StructType.scala:273)",
      "  at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:41)",
      "  at org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:75)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1$$anonfun$4$$anonfun$apply$1.apply$mcD$sp(TrainValidationSplit.scala:157)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1$$anonfun$4$$anonfun$apply$1.apply(TrainValidationSplit.scala:150)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1$$anonfun$4$$anonfun$apply$1.apply(TrainValidationSplit.scala:150)",
      "  at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)",
      "  at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)",
      "  at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)",
      "  at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)",
      "  at scala.concurrent.impl.Future$.apply(Future.scala:31)",
      "  at scala.concurrent.Future$.apply(Future.scala:494)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1$$anonfun$4.apply(TrainValidationSplit.scala:160)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1$$anonfun$4.apply(TrainValidationSplit.scala:149)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit$$anonfun$fit$1.apply(TrainValidationSplit.scala:149)",
      "  ... 43 more",
      ""
     ]
    }
   ],
   "source": [
    "val validatorModel=validator.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val bestModel = validatorModel.bestodel\n",
    "bestModel.aslnstance0f[PipelineModel].stages.last.extractParamMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val paramsAndMetrics = validatorodel.validationMetrics.\n",
    "zip(validatorodel.getEstimatorParamMaps).sortBy(-_._1)\n",
    "paramsAndMetrics.foreachicase (metric,params)=>\n",
    "println(metric)\n",
    "println(params)\n",
    "println()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validatorModel.validationMetrics.max\n",
    "multiclassEval.evaluate(bestHodel.transform(testData))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
